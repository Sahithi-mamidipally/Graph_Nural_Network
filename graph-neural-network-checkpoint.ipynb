{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "## General Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import collections\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import itertools\n",
    "from itertools import combinations \n",
    "\n",
    "# Sklearn Libraries\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "from sklearn import metrics\n",
    " \n",
    "## PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "## PyTorch Geometric Libraries\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import DataLoader as DataLoaderGraph\n",
    "from torch_geometric.data import Dataset as DatasetGraph\n",
    "from torch_geometric.data import Batch as BatchGraph\n",
    "\n",
    "#from torch_geometric.transforms import AddTrainValTestMask as masking\n",
    "\n",
    "from torch_geometric.nn import GCNConv, BatchNorm, SAGEConv, SGConv, ChebConv\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "\n",
    "# NetworkX Library\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section loads the dataset. The dataset includes node features (musae_git_features.json), edges (musae_git_edges.csv), and target labels (musae_git_target.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'musae_git_features.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusae_git_features.json\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_data:\n\u001b[0;32m      2\u001b[0m     data_raw \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_data)\n\u001b[0;32m      4\u001b[0m edges\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusae_git_edges.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'musae_git_features.json'"
     ]
    }
   ],
   "source": [
    "with open(\"musae_git_features.json\") as json_data:\n",
    "    data_raw = json.load(json_data)\n",
    "\n",
    "edges=pd.read_csv(\"musae_git_edges.csv\")\n",
    "target_df=pd.read_csv(\"musae_git_target.csv\")#.to_numpy()[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"5 top nodes labels\")\n",
    "print(target_df.head(5).to_markdown())\n",
    "print()\n",
    "print(\"5 last nodes\")\n",
    "print(target_df.tail(5).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This part explores the dataset by plotting histograms of the target classes, feature counts per node, and overall feature distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats=[]\n",
    "feat_counts=[]\n",
    "for i in range(len(data_raw)):\n",
    "    feat_counts+=[len(data_raw[str(i)])]\n",
    "    feats+=data_raw[str(i)]\n",
    "\n",
    "print(\"5 top nodes labels\")\n",
    "print(target_df.head(5).to_markdown())\n",
    "print()\n",
    "print(\"5 last nodes\")\n",
    "print(target_df.tail(5).to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(target_df.ml_target,bins=4,);\n",
    "plt.title(\"Classes distribution\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(feat_counts,bins=20)\n",
    "plt.title(\"Number of features per graph distribution\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(feats,bins=50)\n",
    "plt.title(\"Features distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=collections.Counter(feats)\n",
    "print(list(counter.keys())[:10])\n",
    "print(list(counter.values())[:10])\n",
    "print(list(counter.keys())[-10:])\n",
    "print(list(counter.values())[-10:])\n",
    "#data_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(light=False,n=60):\n",
    "    if light==True:\n",
    "        nodes_included=n\n",
    "    elif light==False:\n",
    "        nodes_included=len(data_raw)\n",
    "    \n",
    "    data_encoded={}\n",
    "    for i in range(nodes_included):# \n",
    "        one_hot_feat=np.array([0]*(max(feats)+1))\n",
    "        this_feat=data_raw[str(i)]\n",
    "        one_hot_feat[this_feat]=1\n",
    "        data_encoded[str(i)]=list(one_hot_feat)\n",
    "    \n",
    "    if light==True:\n",
    "        sparse_feat_matrix=np.zeros((1,max(feats)+1))\n",
    "        for j in range(nodes_included):\n",
    "            temp=np.array(data_encoded[str(j)]).reshape(1,-1)\n",
    "            sparse_feat_matrix=np.concatenate((sparse_feat_matrix,temp),axis=0)\n",
    "        sparse_feat_matrix=sparse_feat_matrix[1:,:]\n",
    "        return(data_encoded,sparse_feat_matrix)\n",
    "    elif light==False:\n",
    "        return(data_encoded, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, the data is encoded into a one-hot representation suitable for GNN input. The visualization shows a subset of the encoded data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_vis,sparse_feat_matrix_vis=encode_data(light=True,n=60)\n",
    "plt.figure(figsize=(25,25));\n",
    "plt.imshow(sparse_feat_matrix_vis[:,:250],cmap='Greys');\n",
    "#plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(data_encoded,light=False):\n",
    "    node_features_list=list(data_encoded.values())\n",
    "    node_features=torch.tensor(node_features_list)\n",
    "    node_labels=torch.tensor(target_df['ml_target'].values)\n",
    "    edges_list=edges.values.tolist()\n",
    "    edge_index01=torch.tensor(edges_list, dtype = torch.long).T\n",
    "    edge_index02=torch.zeros(edge_index01.shape, dtype = torch.long)#.T\n",
    "    edge_index02[0,:]=edge_index01[1,:]\n",
    "    edge_index02[1,:]=edge_index01[0,:]\n",
    "    edge_index0=torch.cat((edge_index01,edge_index02),axis=1)\n",
    "    g = Data(x=node_features, y=node_labels, edge_index=edge_index0)\n",
    "    g_light = Data(x=node_features[:,0:2],\n",
    "                     y=node_labels   ,\n",
    "                     edge_index=edge_index0[:,:55])\n",
    "    if light:\n",
    "        return(g_light)\n",
    "    else:\n",
    "        return(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This function[draw_graph()] constructs a graph from the encoded data and edges, and optionally plots a small sample of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g_top_draw = Data(x=node_features, y=node_labels, edge_index=edge_index0)\n",
    "\n",
    "def draw_graph(data0):\n",
    "    #node_labels=data0.y\n",
    "    if data0.num_nodes>100:\n",
    "        print(\"This is a big graph, can not plot...\")\n",
    "        return\n",
    "        \n",
    "    else:\n",
    "        data_nx = to_networkx(data0)\n",
    "        node_colors=data0.y[list(data_nx.nodes)]\n",
    "        pos= nx.spring_layout(data_nx,scale =1)\n",
    "        plt.figure(figsize=(12,8))\n",
    "        nx.draw(data_nx, pos,  cmap=plt.get_cmap('Set1'),\n",
    "                node_color =node_colors,node_size=600,connectionstyle=\"angle3\",\n",
    "                width =1, with_labels = False, edge_color = 'k', arrowstyle = \"-\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_sample=construct_graph(data_encoded=data_encoded_vis,light=True)\n",
    "draw_graph(g_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded,_=encode_data(light=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=construct_graph(data_encoded=data_encoded,light=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "msk=T.RandomNodeSplit(split=\"train_rest\", num_splits = 1, num_val = 0.3, num_test= 0.6)\n",
    "g=msk(g)\n",
    "print(g)\n",
    "print()\n",
    "print(\"training samples\",torch.sum(g.train_mask).item())\n",
    "print(\"validation samples\",torch.sum(g.val_mask ).item())\n",
    "print(\"test samples\",torch.sum(g.test_mask ).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This part splits the dataset into training, validation, and test sets using random node split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. GNN Model Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialGNN(torch.nn.Module):\n",
    "    def __init__(self,num_of_feat,f):\n",
    "        super(SocialGNN, self).__init__()\n",
    "\n",
    "\n",
    "        self.conv1 = GCNConv(num_of_feat, f)\n",
    "\n",
    "        self.conv2 = GCNConv(f, 2)\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x.float()\n",
    "        edge_index =  data.edge_index\n",
    "          \n",
    "        x = self.conv1(x=x, edge_index=edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This defines a simple GNN model with two GCN convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Loss and Accuracy Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(predictions,labels,mask):\n",
    "    mask=mask.float()\n",
    "    mask=mask/torch.mean(mask)\n",
    "    loss=criterion(predictions,labels)\n",
    "    loss=loss*mask\n",
    "    loss=torch.mean(loss)\n",
    "    return (loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(predictions,labels,mask):\n",
    "    mask=mask.float()\n",
    "    mask/=torch.mean(mask)\n",
    "    accuracy=(torch.argmax(predictions,axis=1)==labels).long()\n",
    "    accuracy=mask*accuracy\n",
    "    accuracy=torch.mean(accuracy)\n",
    "    return (accuracy)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These functions compute the masked loss and accuracy for the GNN model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Training the GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_social(net,data,epochs=10,lr=0.01):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr) # 00001\n",
    "    best_accuracy=0.0\n",
    "    \n",
    "    train_losses=[]\n",
    "    train_accuracies=[]\n",
    "\n",
    "    val_losses=[]\n",
    "    val_accuracies=[]\n",
    "\n",
    "    test_losses=[]\n",
    "    test_accuracies=[]\n",
    "    \n",
    "    for ep in range(epochs+1):\n",
    "        optimizer.zero_grad()\n",
    "        out=net(data)\n",
    "        loss=masked_loss(predictions=out,\n",
    "                         labels=data.y,\n",
    "                         mask=data.train_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses+=[loss]\n",
    "        train_accuracy=masked_accuracy(predictions=out,\n",
    "                                       labels=data.y, \n",
    "                                       mask=data.train_mask)\n",
    "        train_accuracies+=[train_accuracy]\n",
    "        \n",
    "        val_loss=masked_loss(predictions=out,\n",
    "                             labels=data.y, \n",
    "                             mask=data.val_mask)\n",
    "        val_losses+=[val_loss]\n",
    "        \n",
    "        val_accuracy=masked_accuracy(predictions=out,\n",
    "                                     labels=data.y, \n",
    "                                     mask=data.val_mask)\n",
    "        val_accuracies+=[val_accuracy]\n",
    "\n",
    "        test_accuracy=masked_accuracy(predictions=out,\n",
    "                                      labels=data.y, \n",
    "                                      mask=data.test_mask)\n",
    "        test_accuracies+=[test_accuracy]\n",
    "        if np.round(val_accuracy,4)> np.round(best_accuracy ,4):\n",
    "            print(\"Epoch {}/{}, Train_Loss: {:.4f}, Train_Accuracy: {:.4f}, Val_Accuracy: {:.4f}, Test_Accuracy: {:.4f}\"\n",
    "                      .format(ep+1,epochs, loss.item(), train_accuracy, val_accuracy,  test_accuracy))\n",
    "            best_accuracy=val_accuracy\n",
    "\n",
    "  \n",
    "\n",
    "    \n",
    "    plt.plot(train_accuracies) \n",
    "    plt.plot(val_accuracies)\n",
    "    plt.plot(test_accuracies) \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Best accuracy\",best_accuracy) \n",
    "    return(best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section trains the GNN model on the training data and evaluates it on the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_feat=g.num_node_features\n",
    "net=SocialGNN(num_of_feat=num_of_feat,f=16)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "train_social(net,g,epochs=150,lr=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
